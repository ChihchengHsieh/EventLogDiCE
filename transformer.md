
# Transformer result

### attention weights in each layer and head
![](https://github.com/ChihchengHsieh/LINDA-BN-Eventlog-TF/blob/master/layer_heads_attn.png?raw=true)
#### However, it can be hard to read, I calculate the reduce mean to get result below

### first step
![](https://github.com/ChihchengHsieh/LINDA-BN-Eventlog-TF/blob/master/next_step.png?raw=true)

### Last step
![](https://github.com/ChihchengHsieh/LINDA-BN-Eventlog-TF/blob/master/final_step.png?raw=true)